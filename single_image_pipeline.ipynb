{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run analysis on one Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the localization model\n",
    "weights_path = \"/bask/homes/f/fspo1218/amber/data/mila_models/v1_localizmodel_2021-08-17-12-06.pt\"\n",
    "\n",
    "model_loc = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "num_classes = 2  # 1 class (object) + background\n",
    "in_features = model_loc.roi_heads.box_predictor.cls_score.in_features\n",
    "model_loc.roi_heads.box_predictor = (\n",
    "    torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, num_classes\n",
    "    )\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(weights_path, map_location=device)\n",
    "state_dict = checkpoint.get(\"model_state_dict\") or checkpoint\n",
    "model_loc.load_state_dict(state_dict)\n",
    "model_loc = model_loc.to(device)\n",
    "model_loc.eval()\n",
    "\n",
    "print('localisation model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"/bask/homes/f/fspo1218/amber/data/mila_models/moth-nonmoth-effv2b3_20220506_061527_30.pth\"\n",
    "labels_path = \"/bask/homes/f/fspo1218/amber/data/mila_models/05-moth-nonmoth_category_map.json\"\n",
    "\n",
    "num_classes=2\n",
    "classification_model = timm.create_model(\n",
    "            \"tf_efficientnetv2_b3\",\n",
    "            num_classes=num_classes,\n",
    "            weights=None,\n",
    "        )\n",
    "classification_model = classification_model.to(device)\n",
    "# state_dict = torch.hub.load_state_dict_from_url(weights_url)\n",
    "checkpoint = torch.load(weights_path, map_location=device)\n",
    "# The model state dict is nested in some checkpoints, and not in others\n",
    "state_dict = checkpoint.get(\"model_state_dict\") or checkpoint\n",
    "classification_model.load_state_dict(state_dict)\n",
    "classification_model.eval()\n",
    "\n",
    "print('binary classifier model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),  # Assuming models require 300x300 input images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_species = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((300, 300)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5,0.5, 0.5], std=[0.5,0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = '/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data'\n",
    "\n",
    "all_images = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(image_dir)) for f in fn]\n",
    "all_images = [x for x in all_images if x.endswith('jpg')]\n",
    "\n",
    "\n",
    "print(len(all_images))\n",
    "\n",
    "# print(all_images)\n",
    "\n",
    "# CSV file to save results\n",
    "csv_file = '/bask/projects/v/vjgo8416-amber/projects/object-store-scripts/mila_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = all_images[0:5] \n",
    "all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet50(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: provides parameters for model generation\n",
    "        \"\"\"\n",
    "        super(Resnet50, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = torchvision.models.resnet50(weights=\"DEFAULT\")\n",
    "        out_dim = self.backbone.fc.in_features\n",
    "\n",
    "        self.backbone = torch.nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.classifier = torch.nn.Linear(out_dim, self.num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "weights = '/bask/homes/f/fspo1218/amber/projects/species_classifier/outputs/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt'\n",
    "category_map = json.load(open('/bask/homes/f/fspo1218/amber/data/gbif_costarica/03_costarica_data_category_map.json'))\n",
    "\n",
    "num_classes = len(category_map)\n",
    "species_model = Resnet50(num_classes=num_classes)\n",
    "species_model = species_model.to(device)\n",
    "checkpoint = torch.load(weights, map_location=device)\n",
    "# The model state dict is nested in some checkpoints, and not in others\n",
    "state_dict = checkpoint.get(\"model_state_dict\") or checkpoint\n",
    "\n",
    "species_model.load_state_dict(state_dict)\n",
    "species_model.eval()\n",
    "\n",
    "\n",
    "print('species classifier loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "#from torchvision.models import ResNet50_Weights\n",
    "\n",
    "class ResNet502(nn.Module):\n",
    "    '''ResNet-50 Architecture with pretrained weights\n",
    "    '''\n",
    "\n",
    "    def __init__(self, use_cbam=True, image_depth=3, num_classes=20):\n",
    "        '''Params init and build arch.\n",
    "        '''\n",
    "        super(ResNet502, self).__init__()\n",
    "\n",
    "        self.expansion = 4\n",
    "        self.out_channels = 512\n",
    "        \n",
    "        #self.model_ft = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2) # 80.86, 25.6M\n",
    "        self.model_ft = models.resnet50(pretrained=True)\n",
    "              \n",
    "        # overwrite the 'fc' layer\n",
    "        print(\"In features\", self.model_ft.fc.in_features)\n",
    "        #self.model_ft.fc = nn.Linear(self.model_ft.fc.in_features, 512*self.expansion) \n",
    "        self.model_ft.fc = nn.Identity() # Do nothing just pass input to output\n",
    "        \n",
    "        # At least one layer\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.linear_lvl1 = nn.Linear(self.out_channels*self.expansion, self.out_channels)\n",
    "        self.relu_lv1 = nn.ReLU(inplace=False)\n",
    "        self.softmax_reg1 = nn.Linear(self.out_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward propagation of pretrained ResNet-50.\n",
    "        '''\n",
    "        x = self.model_ft(x)\n",
    "        \n",
    "        x = self.drop(x) # Dropout to add regularization\n",
    "\n",
    "        level_1 = self.softmax_reg1(self.relu_lv1(self.linear_lvl1(x)))\n",
    "        #level_1 = nn.Softmax(level_1)\n",
    "                \n",
    "        return level_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedWeights = '/bask/homes/f/fspo1218/amber/projects/MCC24-trap/model_order_060524/dhc_best_128.pth'\n",
    "thresholdFile = '/bask/homes/f/fspo1218/amber/projects/MCC24-trap/model_order_060524/thresholdsTestTrain.csv'\n",
    "device = 'cpu'\n",
    "img_size = 128\n",
    "        \n",
    "print(\"Order classifier - threshold file\", thresholdFile, \"and weights\", savedWeights, \"of image size\", img_size)\n",
    "\n",
    "data_thresholds = pd.read_csv(thresholdFile)\n",
    "order_labels = data_thresholds[\"ClassName\"].to_list()\n",
    "thresholds = data_thresholds[\"Threshold\"].to_list()\n",
    "means = data_thresholds[\"Mean\"].to_list()\n",
    "stds = data_thresholds[\"Std\"].to_list()\n",
    "\n",
    "img_depth = 3\n",
    "\n",
    "num_classes=len(order_labels)\n",
    "print(\"Use ResNet50 and load weights with num. classes\", num_classes)\n",
    "\n",
    "model_order = ResNet502(num_classes=num_classes) \n",
    "model_order.load_state_dict(torch.load(savedWeights, map_location=device))\n",
    "model_order = model_order.to(device)\n",
    "model_order.eval()\n",
    "\n",
    "print('order classifier loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def classify_order(image_tensor): \n",
    "    augment=False\n",
    "    visualize=False\n",
    "    #visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "    pred = model_order(image_tensor)\n",
    "    \n",
    "\n",
    "    predictions = pred.cpu().detach().numpy()\n",
    "    predicted_label = np.argmax(predictions, axis=1)[0]\n",
    "    print('preds:', predictions)\n",
    "    print('pred labels:', predicted_label)\n",
    "    \n",
    "    label = order_labels[predicted_label]\n",
    "    confidence_value = norm.cdf(predictions[0][predicted_label], \n",
    "                                data_thresholds['Mean'][predicted_label], \n",
    "                                data_thresholds['Std'][predicted_label])\n",
    "    confidence_value = round(confidence_value*10000)/100\n",
    "\n",
    "    return label, confidence_value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_box(image_tensor):\n",
    "    output = classification_model(image_tensor)\n",
    "\n",
    "    predictions = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "    predictions = predictions.detach().numpy()\n",
    "\n",
    "    categories = predictions.argmax(axis=1)\n",
    "\n",
    "    labels = {'moth': 0, 'nonmoth': 1}\n",
    "\n",
    "    index_to_label = {index: label for label, index in labels.items()}\n",
    "\n",
    "    label = [index_to_label[cat] for cat in categories][0]\n",
    "    score = predictions.max(axis=1).astype(float)[0]\n",
    "    return label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_species(image_tensor):  \n",
    "    output = species_model(image_tensor)\n",
    "\n",
    "    predictions = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "    predictions = predictions.detach().numpy()\n",
    "\n",
    "    categories = predictions.argmax(axis=1)\n",
    "    #print(categories)\n",
    "\n",
    "    labels = category_map\n",
    "\n",
    "    index_to_label = {index: label for label, index in labels.items()}\n",
    "\n",
    "    label = [index_to_label[cat] for cat in categories][0]\n",
    "    score = 1 - predictions.max(axis=1).astype(float)[0]\n",
    "    return label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for im_index in range(len(all_images)):\n",
    "\n",
    "    image_path = all_images[im_index]\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    original_image = image.copy()\n",
    "    original_width, original_height = image.size\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    all_boxes = pd.DataFrame(columns=['image_path', \n",
    "                                      'box_score', 'x_min', 'y_min', 'x_max', 'y_max', #localisation info\n",
    "                                      'class_name', 'class_confidence', # binary class info\n",
    "                                      'order_name', 'order_confidence', # order info\n",
    "                                      'species_name', 'species_confidence']) # species info\n",
    "\n",
    "    # Perform object localization\n",
    "    with torch.no_grad():\n",
    "        localization_outputs = model_loc(input_tensor)\n",
    "\n",
    "        print(image_path)\n",
    "        print('Number of objects:', len(localization_outputs[0]['boxes']))\n",
    "\n",
    "        # for each detection\n",
    "        for i in range(len(localization_outputs[0]['boxes'])):\n",
    "            x_min, y_min, x_max, y_max = localization_outputs[0]['boxes'][i]\n",
    "            box_score = localization_outputs[0]['scores'].tolist()[i]\n",
    "\n",
    "            x_min = int(int(x_min) * original_width / 300)\n",
    "            y_min = int(int(y_min) * original_height / 300)\n",
    "            x_max = int(int(x_max) * original_width / 300)\n",
    "            y_max = int(int(y_max) * original_height / 300)\n",
    "\n",
    "            box_width = x_max - x_min\n",
    "            box_height = y_max - y_min\n",
    "\n",
    "            # if box heigh or width > half the image, skip\n",
    "            if box_width > original_width / 2 or box_height > original_height / 2:\n",
    "                continue\n",
    "                \n",
    "            # if confidence below threshold\n",
    "            if box_score <= 0.1:\n",
    "                continue\n",
    "\n",
    "            # Crop the detected region and perform classification\n",
    "            cropped_image = original_image.crop((x_min, y_min, x_max, y_max))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0)\n",
    "\n",
    "            class_name, class_confidence = classify_box(cropped_tensor)            \n",
    "            order_name, order_confidence = classify_order(cropped_tensor)  \n",
    "            \n",
    "\n",
    "            # Annotate image with bounding box and class\n",
    "            if class_name == 'moth':\n",
    "                # Perform the species classification\n",
    "                print('...Performing the inference')\n",
    "                species_name, species_confidence = classify_species(cropped_tensor)\n",
    "\n",
    "                draw = ImageDraw.Draw(original_image)\n",
    "                draw.rectangle([x_min, y_min, x_max, y_max], outline='green', width=3)\n",
    "                draw.text((x_min, y_min - 10), species_name + \" , %.3f \" % species_confidence, fill='green')\n",
    "\n",
    "            else:\n",
    "                species_name, species_confidence = None, None\n",
    "                draw = ImageDraw.Draw(original_image)\n",
    "                draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n",
    "                draw.text((x_min, y_min - 10), f'order: {order_name}, binary: {class_name}', fill='red')\n",
    "\n",
    "            draw.text((x_min, y_max), str(box_score), fill='black')\n",
    "\n",
    "            # append to csv with pandas\n",
    "            df = pd.DataFrame([[image_path, \n",
    "                                box_score, x_min, y_min, x_max, y_max, \n",
    "                                class_name, class_confidence ,\n",
    "                                order_name, order_confidence,\n",
    "                                species_name, species_confidence]],\n",
    "                              columns=['image_path', \n",
    "                                      'box_score', 'x_min', 'y_min', 'x_max', 'y_max', \n",
    "                                      'class_name', 'class_confidence', \n",
    "                                      'order_name', 'order_confidence',\n",
    "                                      'species_name', 'species_confidence'])\n",
    "            all_boxes = pd.concat([all_boxes, df])\n",
    "            df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "        # if (all_boxes['class'] == 'moth').any():\n",
    "            # print('...Moth Detected')\n",
    "        original_image.save(os.path.basename(image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_conda_env2 (Conda)",
   "language": "python",
   "name": "sys_kg_conda_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
