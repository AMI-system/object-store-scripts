{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run analysis on one Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localisation model loaded\n"
     ]
    }
   ],
   "source": [
    "# load in the localization model\n",
    "weights_path = \"/bask/homes/f/fspo1218/amber/data/mila_models/v1_localizmodel_2021-08-17-12-06.pt\"\n",
    "\n",
    "model_loc = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "num_classes = 2  # 1 class (object) + background\n",
    "in_features = model_loc.roi_heads.box_predictor.cls_score.in_features\n",
    "model_loc.roi_heads.box_predictor = (\n",
    "    torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, num_classes\n",
    "    )\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(weights_path, map_location=device)\n",
    "state_dict = checkpoint.get(\"model_state_dict\") or checkpoint\n",
    "model_loc.load_state_dict(state_dict)\n",
    "model_loc = model_loc.to(device)\n",
    "model_loc.eval()\n",
    "\n",
    "print('localisation model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary classifier model loaded\n"
     ]
    }
   ],
   "source": [
    "weights_path = \"/bask/homes/f/fspo1218/amber/data/mila_models/moth-nonmoth-effv2b3_20220506_061527_30.pth\"\n",
    "labels_path = \"/bask/homes/f/fspo1218/amber/data/mila_models/05-moth-nonmoth_category_map.json\"\n",
    "\n",
    "num_classes=2\n",
    "classification_model = timm.create_model(\n",
    "            \"tf_efficientnetv2_b3\",\n",
    "            num_classes=num_classes,\n",
    "            weights=None,\n",
    "        )\n",
    "classification_model = classification_model.to(device)\n",
    "# state_dict = torch.hub.load_state_dict_from_url(weights_url)\n",
    "checkpoint = torch.load(weights_path, map_location=device)\n",
    "# The model state dict is nested in some checkpoints, and not in others\n",
    "state_dict = checkpoint.get(\"model_state_dict\") or checkpoint\n",
    "classification_model.load_state_dict(state_dict)\n",
    "classification_model.eval()\n",
    "\n",
    "print('binary classifier model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11827\n"
     ]
    }
   ],
   "source": [
    "# Transformations for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),  # Assuming models require 300x300 input images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_species = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((300, 300)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5,0.5, 0.5], std=[0.5,0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = '/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data'\n",
    "\n",
    "all_images = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(image_dir)) for f in fn]\n",
    "all_images = [x for x in all_images if x.endswith('jpg')]\n",
    "\n",
    "\n",
    "print(len(all_images))\n",
    "\n",
    "# print(all_images)\n",
    "\n",
    "# CSV file to save results\n",
    "csv_file = '/bask/projects/v/vjgo8416-amber/projects/object-store-scripts/mila_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data/dep000036/snapshot_images/01-20240205220009-snapshot.jpg',\n",
       " '/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data/dep000036/snapshot_images/01-20240206031249-snapshot.jpg',\n",
       " '/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data/dep000036/snapshot_images/01-20240205222839-snapshot.jpg',\n",
       " '/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data/dep000036/snapshot_images/01-20240206030239-snapshot.jpg',\n",
       " '/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data/dep000036/snapshot_images/01-20240206001719-snapshot.jpg']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images = all_images[0:5] #['/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data2/dep000032/snapshot_images/01-20240131184010-snapshot.jpg',\n",
    "#'/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data2/dep000032/snapshot_images/01-20240131183850-snapshot.jpg',\n",
    "#'/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data2/dep000032/snapshot_images/01-20240131183800-snapshot.jpg',\n",
    "#'/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data2/dep000032/snapshot_images/01-20240131183720-snapshot.jpg']\n",
    "all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet50(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: provides parameters for model generation\n",
    "        \"\"\"\n",
    "        super(Resnet50, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = torchvision.models.resnet50(weights=\"DEFAULT\")\n",
    "        out_dim = self.backbone.fc.in_features\n",
    "\n",
    "        self.backbone = torch.nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.classifier = torch.nn.Linear(out_dim, self.num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species classifier loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "weights = '/bask/homes/f/fspo1218/amber/projects/species_classifier/outputs/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt'\n",
    "category_map = json.load(open('/bask/homes/f/fspo1218/amber/data/gbif_costarica/03_costarica_data_category_map.json'))\n",
    "\n",
    "num_classes = len(category_map)\n",
    "species_model = Resnet50(num_classes=num_classes)\n",
    "species_model = species_model.to(device)\n",
    "checkpoint = torch.load(weights, map_location=device)\n",
    "# The model state dict is nested in some checkpoints, and not in others\n",
    "state_dict = checkpoint.get(\"model_state_dict\") or checkpoint\n",
    "\n",
    "species_model.load_state_dict(state_dict)\n",
    "species_model.eval()\n",
    "\n",
    "\n",
    "print('species classifier loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "#from torchvision.models import ResNet50_Weights\n",
    "\n",
    "class ResNet502(nn.Module):\n",
    "    '''ResNet-50 Architecture with pretrained weights\n",
    "    '''\n",
    "\n",
    "    def __init__(self, use_cbam=True, image_depth=3, num_classes=20):\n",
    "        '''Params init and build arch.\n",
    "        '''\n",
    "        super(ResNet502, self).__init__()\n",
    "\n",
    "        self.expansion = 4\n",
    "        self.out_channels = 512\n",
    "        \n",
    "        #self.model_ft = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2) # 80.86, 25.6M\n",
    "        self.model_ft = models.resnet50(pretrained=True)\n",
    "              \n",
    "        # overwrite the 'fc' layer\n",
    "        print(\"In features\", self.model_ft.fc.in_features)\n",
    "        #self.model_ft.fc = nn.Linear(self.model_ft.fc.in_features, 512*self.expansion) \n",
    "        self.model_ft.fc = nn.Identity() # Do nothing just pass input to output\n",
    "        \n",
    "        # At least one layer\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.linear_lvl1 = nn.Linear(self.out_channels*self.expansion, self.out_channels)\n",
    "        self.relu_lv1 = nn.ReLU(inplace=False)\n",
    "        self.softmax_reg1 = nn.Linear(self.out_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward propagation of pretrained ResNet-50.\n",
    "        '''\n",
    "        x = self.model_ft(x)\n",
    "        \n",
    "        x = self.drop(x) # Dropout to add regularization\n",
    "\n",
    "        level_1 = self.softmax_reg1(self.relu_lv1(self.linear_lvl1(x)))\n",
    "        #level_1 = nn.Softmax(level_1)\n",
    "                \n",
    "        return level_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order classifier - threshold file /bask/homes/f/fspo1218/amber/projects/MCC24-trap/model_order_060524/thresholdsTestTrain.csv and weights /bask/homes/f/fspo1218/amber/projects/MCC24-trap/model_order_060524/dhc_best_128.pth of image size 128\n",
      "Use ResNet50 and load weights with num. classes 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bask/homes/f/fspo1218/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/bask/homes/f/fspo1218/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In features 2048\n",
      "order classifier loaded\n"
     ]
    }
   ],
   "source": [
    "savedWeights = '/bask/homes/f/fspo1218/amber/projects/MCC24-trap/model_order_060524/dhc_best_128.pth'\n",
    "thresholdFile = '/bask/homes/f/fspo1218/amber/projects/MCC24-trap/model_order_060524/thresholdsTestTrain.csv'\n",
    "device = 'cpu'\n",
    "img_size = 128\n",
    "        \n",
    "print(\"Order classifier - threshold file\", thresholdFile, \"and weights\", savedWeights, \"of image size\", img_size)\n",
    "\n",
    "data_thresholds = pd.read_csv(thresholdFile)\n",
    "order_labels = data_thresholds[\"ClassName\"].to_list()\n",
    "thresholds = data_thresholds[\"Threshold\"].to_list()\n",
    "means = data_thresholds[\"Mean\"].to_list()\n",
    "stds = data_thresholds[\"Std\"].to_list()\n",
    "\n",
    "img_depth = 3\n",
    "\n",
    "num_classes=len(order_labels)\n",
    "print(\"Use ResNet50 and load weights with num. classes\", num_classes)\n",
    "\n",
    "model_order = ResNet502(num_classes=num_classes) \n",
    "model_order.load_state_dict(torch.load(savedWeights, map_location=device))\n",
    "model_order = model_order.to(device)\n",
    "model_order.eval()\n",
    "\n",
    "print('order classifier loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def classify_order(image_tensor): \n",
    "    augment=False\n",
    "    visualize=False\n",
    "    #visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "    pred = model_order(image_tensor)\n",
    "    \n",
    "\n",
    "    predictions = pred.cpu().detach().numpy()\n",
    "    predicted_label = np.argmax(predictions, axis=1)[0]\n",
    "    print('preds:', predictions)\n",
    "    print('pred labels:', predicted_label)\n",
    "    \n",
    "    label = order_labels[predicted_label]\n",
    "    confidence_value = norm.cdf(predictions[0][predicted_label], \n",
    "                                data_thresholds['Mean'][predicted_label], \n",
    "                                data_thresholds['Std'][predicted_label])\n",
    "    confidence_value = round(confidence_value*10000)/100\n",
    "\n",
    "    return label, confidence_value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_box(image_tensor):\n",
    "    output = classification_model(image_tensor)\n",
    "\n",
    "    predictions = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "    predictions = predictions.detach().numpy()\n",
    "\n",
    "    categories = predictions.argmax(axis=1)\n",
    "\n",
    "    labels = {'moth': 0, 'nonmoth': 1}\n",
    "\n",
    "    index_to_label = {index: label for label, index in labels.items()}\n",
    "\n",
    "    label = [index_to_label[cat] for cat in categories][0]\n",
    "    score = predictions.max(axis=1).astype(float)[0]\n",
    "    return label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_species(image_tensor):  \n",
    "    output = species_model(image_tensor)\n",
    "\n",
    "    predictions = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "    predictions = predictions.detach().numpy()\n",
    "\n",
    "    categories = predictions.argmax(axis=1)\n",
    "    #print(categories)\n",
    "\n",
    "    labels = category_map\n",
    "\n",
    "    index_to_label = {index: label for label, index in labels.items()}\n",
    "\n",
    "    label = [index_to_label[cat] for cat in categories][0]\n",
    "    score = 1 - predictions.max(axis=1).astype(float)[0]\n",
    "    return label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data/dep000036/snapshot_images/01-20240205220009-snapshot.jpg\n",
      "Number of objects: 3\n",
      "preds: [[ 0.44072843 -1.6463149  -1.5811099   0.8215571  -0.65998477 -1.114519\n",
      "  -1.1585019  -1.0904424  -2.0323513  -2.2101672   1.7512808   1.8586932\n",
      "  -0.54696345 -0.11473865 -0.43083093  0.2399748 ]]\n",
      "pred labels: 11\n",
      "/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data/dep000036/snapshot_images/01-20240206031249-snapshot.jpg\n",
      "Number of objects: 6\n",
      "preds: [[ 0.43966907 -1.1814544   0.4656911   1.5476805  -1.3775582  -1.578891\n",
      "  -0.9646145  -0.41996062 -1.8940152  -2.2265093   0.31332022  1.5981374\n",
      "  -0.73623705 -1.4390609  -0.54086864 -0.7314318 ]]\n",
      "pred labels: 11\n",
      "...Performing the inference\n",
      "preds: [[-0.326524   -1.9070983  -1.9405364   2.061741   -1.4876083  -2.3633442\n",
      "  -0.15144947 -1.252148   -1.7541928  -2.9306302   3.4133132   1.2911898\n",
      "  -1.1646793  -1.6377012  -0.06715114 -1.1000919 ]]\n",
      "pred labels: 10\n",
      "...Performing the inference\n",
      "preds: [[-0.4062883  -3.7887638  -5.7694654   0.9730315  -0.17393467 -0.9363236\n",
      "  -0.94114864 -2.1671994  -1.8201343  -3.6937625  -1.5202532  -1.8551096\n",
      "   0.07773536  4.673947    2.6854131   0.316007  ]]\n",
      "pred labels: 13\n",
      "preds: [[ 0.24351835 -1.4179977  -0.7024067   0.1355107  -1.5469519  -1.8509399\n",
      "  -1.1085429  -1.1427201  -2.3948662  -1.9639059   2.6195557   3.200332\n",
      "  -0.65458685 -1.167302   -0.76668507  0.29440778]]\n",
      "pred labels: 11\n",
      "...Performing the inference\n",
      "preds: [[-1.4966313  -1.9613427  -4.7999244  -0.00596255 -0.644082   -2.797582\n",
      "  -1.8653758  -2.3514998  -2.533595   -2.6814125   2.3283587  -1.3894365\n",
      "   0.55495864 -0.08392621  0.40999404  3.3583097 ]]\n",
      "pred labels: 15\n",
      "/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data/dep000036/snapshot_images/01-20240205222839-snapshot.jpg\n",
      "Number of objects: 1\n",
      "preds: [[-0.1471209  -1.8977681  -1.5138748  -0.01488022 -1.5845128  -2.3359835\n",
      "  -0.27763343 -0.8547134  -2.6255863  -1.8665584   2.747124    2.7151427\n",
      "  -0.23828074 -1.0144818  -0.84805334  0.7936657 ]]\n",
      "pred labels: 10\n",
      "/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data/dep000036/snapshot_images/01-20240206030239-snapshot.jpg\n",
      "Number of objects: 11\n",
      "preds: [[ 0.75200725 -2.3127053  -1.2955728   0.8610113  -1.1254344  -1.6858128\n",
      "  -0.60296524 -1.6590184  -2.5585854  -2.455971    1.843223    2.2256703\n",
      "  -1.0264045  -0.44407326 -0.28110704  0.139097  ]]\n",
      "pred labels: 11\n",
      "...Performing the inference\n",
      "preds: [[-7.5609660e-01 -7.2119540e-01 -2.2639625e+00 -6.7072976e-03\n",
      "  -2.2244561e+00 -3.4940882e+00 -1.9622840e+00 -1.7607003e+00\n",
      "  -2.6582794e+00 -2.8757777e+00  7.9453263e+00  3.6064327e+00\n",
      "  -1.7094277e+00 -3.9698255e+00 -7.2007990e-01 -7.9470789e-01]]\n",
      "pred labels: 10\n",
      "...Performing the inference\n",
      "preds: [[-0.3714178  -1.5865417  -2.082402    0.6782063  -2.0014174  -2.3773863\n",
      "  -0.83371514 -1.5267925  -2.0275123  -2.4148939   4.428865    2.4803605\n",
      "  -0.77124876 -1.4916458   0.32752755 -0.5844932 ]]\n",
      "pred labels: 10\n",
      "...Performing the inference\n",
      "preds: [[-0.51246965 -3.7961862  -5.4197593   0.5808812  -0.45699614 -1.3284714\n",
      "  -1.1563547  -2.0738342  -2.1522202  -3.409401   -0.6431551  -1.106029\n",
      "   0.49361065  3.796194    2.4879017   0.81522095]]\n",
      "pred labels: 13\n",
      "...Performing the inference\n",
      "preds: [[ 1.3134052   0.35053408  0.2200874   2.599975   -0.22444102 -1.5520774\n",
      "  -1.0128082   1.5003582  -0.4979757  -2.5793643  -1.4676172  -1.9824498\n",
      "  -1.9351848  -0.6712777  -1.4038318  -1.7130215 ]]\n",
      "pred labels: 3\n",
      "...Performing the inference\n",
      "preds: [[-0.11866438 -2.838263   -3.8388398   3.0718958  -0.07778902 -0.7934063\n",
      "   0.0144559  -0.7140805  -0.91879845 -3.4951003  -1.8177611  -2.2684596\n",
      "  -0.59957135  2.1876464   1.510106   -1.1086661 ]]\n",
      "pred labels: 3\n",
      "preds: [[ 0.08560587 -0.34703773 -0.8663725   1.2496095  -1.6697071  -2.7418718\n",
      "  -1.4837387  -1.0108644  -1.7581494  -2.4933155   5.0995135   1.3908495\n",
      "  -2.330884   -3.1320653  -1.0374404  -1.5154201 ]]\n",
      "pred labels: 10\n",
      "...Performing the inference\n",
      "/bask/homes/f/fspo1218/amber/projects/object-store-scripts/data/dep000036/snapshot_images/01-20240206001719-snapshot.jpg\n",
      "Number of objects: 3\n",
      "preds: [[-0.22011723 -2.31535    -2.6351829   1.8982369  -0.6907958  -0.6692736\n",
      "  -1.2154294  -1.5251602  -1.754542   -2.7050514   1.7335924   1.4174994\n",
      "  -0.51980937  0.01138618  0.90511817 -0.68777895]]\n",
      "pred labels: 3\n",
      "preds: [[ 0.45260924 -1.109545   -0.25043786  2.6236384  -0.87720644 -2.1909788\n",
      "  -1.1859379   0.80197066 -1.2919238  -2.2196894  -0.26701218 -1.3283274\n",
      "  -1.1924106  -1.2548709  -0.31394893 -0.63803995]]\n",
      "pred labels: 3\n",
      "preds: [[ 0.4068973  -0.7392585   0.33034277  2.418588   -0.7631388  -2.102247\n",
      "   0.28942698 -0.22033149 -0.5250009  -2.7898812   0.11336604  0.455703\n",
      "  -2.0053887  -1.8395716  -0.71264094 -1.5438427 ]]\n",
      "pred labels: 3\n",
      "...Performing the inference\n"
     ]
    }
   ],
   "source": [
    "for im_index in range(len(all_images)):\n",
    "\n",
    "    image_path = all_images[im_index]\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    original_image = image.copy()\n",
    "    original_width, original_height = image.size\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    all_boxes = pd.DataFrame(columns=['image_path', \n",
    "                                      'box_score', 'x_min', 'y_min', 'x_max', 'y_max', #localisation info\n",
    "                                      'class_name', 'class_confidence', # binary class info\n",
    "                                      'order_name', 'order_confidence', # order info\n",
    "                                      'species_name', 'species_confidence']) # species info\n",
    "\n",
    "    # Perform object localization\n",
    "    with torch.no_grad():\n",
    "        localization_outputs = model_loc(input_tensor)\n",
    "\n",
    "        print(image_path)\n",
    "        print('Number of objects:', len(localization_outputs[0]['boxes']))\n",
    "\n",
    "        # for each detection\n",
    "        for i in range(len(localization_outputs[0]['boxes'])):\n",
    "            x_min, y_min, x_max, y_max = localization_outputs[0]['boxes'][i]\n",
    "            box_score = localization_outputs[0]['scores'].tolist()[i]\n",
    "\n",
    "            x_min = int(int(x_min) * original_width / 300)\n",
    "            y_min = int(int(y_min) * original_height / 300)\n",
    "            x_max = int(int(x_max) * original_width / 300)\n",
    "            y_max = int(int(y_max) * original_height / 300)\n",
    "\n",
    "            box_width = x_max - x_min\n",
    "            box_height = y_max - y_min\n",
    "\n",
    "            # if box heigh or width > half the image, skip\n",
    "            if box_width > original_width / 2 or box_height > original_height / 2:\n",
    "                continue\n",
    "                \n",
    "            # if confidence below threshold\n",
    "            if box_score <= 0.1:\n",
    "                continue\n",
    "\n",
    "            # Crop the detected region and perform classification\n",
    "            cropped_image = original_image.crop((x_min, y_min, x_max, y_max))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0)\n",
    "\n",
    "            class_name, class_confidence = classify_box(cropped_tensor)            \n",
    "            order_name, order_confidence = classify_order(cropped_tensor)  \n",
    "            \n",
    "\n",
    "            # Annotate image with bounding box and class\n",
    "            if class_name == 'moth':\n",
    "                # Perform the species classification\n",
    "                print('...Performing the inference')\n",
    "                species_name, species_confidence = classify_species(cropped_tensor)\n",
    "\n",
    "                draw = ImageDraw.Draw(original_image)\n",
    "                draw.rectangle([x_min, y_min, x_max, y_max], outline='green', width=3)\n",
    "                draw.text((x_min, y_min - 10), species_name + \" , %.3f \" % species_confidence, fill='green')\n",
    "\n",
    "            else:\n",
    "                species_name, species_confidence = None, None\n",
    "                draw = ImageDraw.Draw(original_image)\n",
    "                draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n",
    "                draw.text((x_min, y_min - 10), f'order: {order_name}, binary: {class_name}', fill='red')\n",
    "\n",
    "            draw.text((x_min, y_max), str(box_score), fill='black')\n",
    "\n",
    "            # append to csv with pandas\n",
    "            df = pd.DataFrame([[image_path, \n",
    "                                box_score, x_min, y_min, x_max, y_max, \n",
    "                                class_name, class_confidence ,\n",
    "                                order_name, order_confidence,\n",
    "                                species_name, species_confidence]],\n",
    "                              columns=['image_path', \n",
    "                                      'box_score', 'x_min', 'y_min', 'x_max', 'y_max', \n",
    "                                      'class_name', 'class_confidence', \n",
    "                                      'order_name', 'order_confidence',\n",
    "                                      'species_name', 'species_confidence'])\n",
    "            all_boxes = pd.concat([all_boxes, df])\n",
    "            df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "        # if (all_boxes['class'] == 'moth').any():\n",
    "            # print('...Moth Detected')\n",
    "        original_image.save(os.path.basename(image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_conda_env2 (Conda)",
   "language": "python",
   "name": "sys_kg_conda_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
